{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ducanh.trinh\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.manifold import MDS, TSNE, LocallyLinearEmbedding, Isomap\n",
    "from MulticoreTSNE import MulticoreTSNE\n",
    "from sklearn.decomposition import TruncatedSVD, PCA, KernelPCA, SparsePCA\n",
    "import umap\n",
    "from scipy.sparse import vstack\n",
    "import pandas as pd\n",
    "from gb_writer import GlyphboardWriter\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from typing import Any\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "SGD = SGDClassifier(loss=\"modified_huber\", penalty='l2', alpha=1e-3,\n",
    "                    random_state=42, max_iter=5, tol=None)\n",
    "MNB = MultinomialNB()\n",
    "LR = LogisticRegression()\n",
    "SVC = LinearSVC()\n",
    "KNC = KNeighborsClassifier()\n",
    "NC = NearestCentroid()\n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "vec = TfidfVectorizer(strip_accents='ascii', max_df=0.5, sublinear_tf=True)\n",
    "SPLICE_POINT = 800\n",
    "UNLABELED_VALUE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    print('Updating JSON...')\n",
    "    updateDatasetJson()\n",
    "    # print('Cleaning Texts...')\n",
    "    # cleanupTexts()\n",
    "    print('Done')\n",
    "\n",
    "def mockInit():\n",
    "    texts = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    peer_labels = []\n",
    "    with open(\"test_data.json\", \"r\") as read_file:\n",
    "        LC_data = json.load(read_file)\n",
    "\n",
    "    for doc in LC_data:\n",
    "        ids.append(doc['id'])\n",
    "        texts.append(doc[\"values\"][\"7\"])\n",
    "        peer_labels.append(doc[\"features\"][\"1\"][\"4\"])\n",
    "        # simulate all as labeled (for test_data)\n",
    "        if (doc[\"features\"][\"1\"][\"4\"] > 0.5):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'text': texts,\n",
    "        'label': labels,\n",
    "        'peer_label': peer_labels,\n",
    "        'score': [0] * len(LC_data),\n",
    "        'isLabeled': [0]  * len(LC_data),\n",
    "        'entities': [' '] * len(LC_data)\n",
    "    })\n",
    "   \n",
    "    test_data = df[SPLICE_POINT+1:]\n",
    "    saveData(test_data, 'test_data')\n",
    "    # test_data.to_csv('test_data.csv', sep=\";\", encoding=\"utf8\", index=False)\n",
    "    data_with_scores = getSelectionScores(rest_data=df, train_data=test_data)\n",
    "    saveData(data_with_scores)\n",
    "    resetTrainData()\n",
    "    \n",
    "    # data_with_scores.to_csv('data.csv', sep=\";\", encoding=\"utf8\", index=False)\n",
    "    # resetTrainData()\n",
    "\n",
    "def loadData(name = 'data'):\n",
    "    return pd.read_csv('{}.csv'.format(name), sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "def saveData(data, name = 'data'):\n",
    "    with open('{}.csv'.format(name), mode='w', newline='\\n', encoding='utf-8') as f:\n",
    "        data.to_csv(f, sep=\";\", line_terminator='\\n', encoding='utf-8', index=False)\n",
    "    # data.to_csv('mlbackend/{}.csv'.format(name), sep=\";\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "def handleNewAnswer(answer):\n",
    "    newAnswer = {\n",
    "        'text': answer['text'],\n",
    "        'docId': answer['documentId'],\n",
    "        'label': int(answer['answer']),\n",
    "        'question': answer['questionId']\n",
    "    }\n",
    "    train_data = getTrainData()\n",
    "\n",
    "    test_data = getTestData()\n",
    "    \n",
    "    data = updateDataWithLabel(loadData(), newAnswer['docId'], newAnswer['label'])\n",
    "    if len(train_data) > 3:\n",
    "        # tfidf = vec.fit_transform(data.text)        \n",
    "        # positions = applyDR(tfidf, withPreviousPos=False, labels=data.label)\n",
    "        # writer = GlyphboardWriter('test_name')\n",
    "        # position_response = writer.write_position(positions=positions, algorithm='umap')\n",
    "        train_result = train(train_data, test_data, SGD)\n",
    "        return {\n",
    "            # 'positions': position_response,\n",
    "            'train_result': train_result\n",
    "        }\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def handleCompleteUpdate():\n",
    "    data = loadData()\n",
    "    # updateDatasetJson()\n",
    "    tfidf = vec.fit_transform(data.text)\n",
    "    positions = applyDR(tfidf, withPreviousPos=True, labels=data.label)\n",
    "    writer = GlyphboardWriter('test_name')\n",
    "    position_response = writer.write_position(positions=positions, algorithm='umap')\n",
    "    return position_response\n",
    "\n",
    "def updateDatasetJson():\n",
    "    with open(\"test_data.json\", \"r\") as read_file:\n",
    "        LC_data = json.load(read_file)\n",
    "        \n",
    "    data = loadData()\n",
    "\n",
    "    for doc in LC_data:\n",
    "        doc['features']['1']['31'] = int(data.loc[data['id'] == doc['id']].isLabeled.values[0])\n",
    "        doc['values']['31'] = int(data.loc[data['id'] == doc['id']].isLabeled.values[0])\n",
    "        doc['features']['1']['32'] = float(data.loc[data['id'] == doc['id']].score.values[0])\n",
    "        doc['values']['32'] = float(data.loc[data['id'] == doc['id']].score.values[0])\n",
    "        doc['features']['1']['33'] = int(data.loc[data['id'] == doc['id']].label.values[0])\n",
    "        doc['values']['33'] = int(data.loc[data['id'] == doc['id']].label.values[0])\n",
    "        doc['features']['1']['34'] = str(data.loc[data['id'] == doc['id']].entities.values[0])\n",
    "        doc['values']['34'] = str(data.loc[data['id'] == doc['id']].entities.values[0])\n",
    "\n",
    "    with open(\"../backend/data/mainTfIdf/mainTfIdf.05112018.feature.json\", \"w\") as f:\n",
    "            json.dump(LC_data, f)\n",
    "    \n",
    "    return 'Done'\n",
    "\n",
    "def updateDataWithLabel(data, docId, label):\n",
    "    print('before', data.loc[data['id'] == docId])\n",
    "    data.loc[data['id'] == docId, 'label'] = int(label)\n",
    "    data.loc[data['id'] == docId, 'isLabeled'] = 1\n",
    "    print('after', data.loc[data['id'] == docId])\n",
    "    saveData(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def createMetrics(algo, train_data):\n",
    "    test_data = getTestData()\n",
    "    met = []\n",
    "    # Create stepwise metrics algo, simulating a history\n",
    "    for number in range(30, len(train_data)):\n",
    "        train_data_iteration = train_data.head(number)\n",
    "        met.append(train(train_data_iteration, test_data, algo=algo))\n",
    "    return pd.DataFrame(met)\n",
    "\n",
    "\n",
    "def train(train_data, test_data, algo: Any) -> dict:\n",
    "    text_clf = Pipeline([\n",
    "        # ('vect', CountVectorizer()),\n",
    "        ('tfidf', vec),\n",
    "        ('clf', algo),\n",
    "    ])\n",
    "    text_clf.fit(train_data.text, train_data.label)\n",
    "    predicted = text_clf.predict(test_data.text)\n",
    "    addHistory(metrics.f1_score(test_data.label, predicted))\n",
    "    result = {\n",
    "        'precision': metrics.precision_score(test_data.label, predicted),\n",
    "        'recall': metrics.recall_score(test_data.label, predicted),\n",
    "        'f1': metrics.f1_score(test_data.label, predicted),\n",
    "        'f1_history': getHistory()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def getTrainData():\n",
    "    data = loadData()\n",
    "    return data.loc[data['isLabeled'] == 1]\n",
    "\n",
    "def getTestData():\n",
    "    return pd.read_csv('test_data.csv', delimiter=';', encoding=\"utf8\")\n",
    "\n",
    "def resetTrainData():\n",
    "    data = loadData()\n",
    "    data.loc[:, 'label'] = UNLABELED_VALUE\n",
    "    data.loc[:, 'isLabeled'] = 0\n",
    "    saveData(data)\n",
    "\n",
    "def cleanupTexts():\n",
    "    data = loadData()\n",
    "    for idx, text in enumerate(data.text):\n",
    "        data.loc[idx, 'text'] = preprocessText(text)\n",
    "        data.loc[idx, 'entities'] = extractNER(text)\n",
    "        \n",
    "    saveData(data)\n",
    "\n",
    "def mockTraining(amount):\n",
    "    data = loadData()\n",
    "    for i in range(amount):\n",
    "        data.loc[i, 'isLabeled'] = 1\n",
    "        if data.loc[i].peer_label > 0.5:\n",
    "            data.loc[i, 'label'] = 1\n",
    "        else:\n",
    "            data.loc[i, 'label'] = 0        \n",
    "    saveData(data)\n",
    "\n",
    "def simulateTraining(iterations):\n",
    "    test_data = getTestData()\n",
    "    mockTraining(iterations)\n",
    "    train_data = getTrainData()\n",
    "    train(train_data, test_data, SGD)\n",
    "\n",
    "def getHistory():\n",
    "    history = []\n",
    "    with open(\n",
    "            \"metrics.csv\", \"r\", encoding=\"utf8\") as file:\n",
    "        reader = csv.reader(file, delimiter=';')\n",
    "        for line in reader:\n",
    "            history.append(line[0])\n",
    "        file.close()\n",
    "    return history\n",
    "\n",
    "\n",
    "def addHistory(metrics):\n",
    "    with open(\n",
    "            \"metrics.csv\", \"a\",  newline=\"\", encoding=\"utf8\") as file:\n",
    "        writer = csv.writer(file, delimiter=';')\n",
    "        writer.writerow([str(metrics)])\n",
    "        file.close()\n",
    "\n",
    "def getCurrentScore() -> int:\n",
    "    return getHistory().pop()\n",
    "\n",
    "def applyDR(tfidf, labels = [], withPreviousPos = True, factor = 1):    \n",
    "    # pre_computed = TruncatedSVD(n_components=100, random_state=1).fit_transform(tfidf.toarray())\n",
    "    # LABEL_IMPACT = 0\n",
    "    if withPreviousPos:        \n",
    "        previousPositions = loadData('previousPositions').values\n",
    "    else:\n",
    "        previousPositions = 'spectral'\n",
    "    labels_arr = np.asarray(labels)\n",
    "    labels_arr = labels_arr.reshape(len(labels_arr), 1)\n",
    "    # with_labels = np.hstack((tfidf.toarray(), labels_arr))\n",
    "    computed_coords = umap.UMAP(init=previousPositions,min_dist=0.8, random_state=1, learning_rate=0.5).fit(tfidf.toarray(), y=labels_arr)\n",
    "    computed_coords = computed_coords.embedding_\n",
    "    saveData(pd.DataFrame(computed_coords), 'previousPositions')\n",
    "    computed_coords *= factor    \n",
    "    # computed_coords = MulticoreTSNE(n_jobs=4, random_state=1).fit_transform(with_labels)\n",
    "    df = pd.DataFrame(columns=['x', 'y'])\n",
    "    df['x'] = computed_coords[:, 0]\n",
    "    df['y'] = computed_coords[:, 1]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocessText(text: str) -> str:\n",
    "    # print('Original: ', text)\n",
    "    doc = nlp(text)\n",
    "    # Remove Stop Words and get Lemmas\n",
    "    return ' '.join([token.text for token in doc if not token.is_stop])\n",
    "    \n",
    "def extractNER(text):\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        cleaned = ent.text.replace('\\r', '')\n",
    "        cleaned = cleaned.replace('\\n', '')\n",
    "        entities.append(cleaned)\n",
    "    if (len(entities) > 0):\n",
    "        return ', '.join(entities)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def getSelectionScores(rest_data, train_data, clf = MNB): \n",
    "    text_clf = Pipeline([\n",
    "        # ('vect', CountVectorizer()),\n",
    "        ('tfidf', vec),\n",
    "        ('clf', clf),\n",
    "    ])\n",
    "    text_clf.fit(train_data.text, train_data.label)\n",
    "    prs = text_clf.predict_proba(rest_data.text) \n",
    "    result_pos = [1-2*abs(x[1]-0.5)  for x in prs]\n",
    "    rest_data['score'] = result_pos\n",
    "    return rest_data\n",
    "    \n",
    "def analyseImportantFeatures(clf=SGD):\n",
    "    train_data = getTrainData()\n",
    "    tfidf = vec.fit_transform(train_data.text)\n",
    "    clf.fit(tfidf, train_data.label)\n",
    "    feature_names = vec.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:20], coefs_with_fns[:-(20 + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print('\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s' % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_position(positions, algorithm):\n",
    "    pos = []\n",
    "\n",
    "    for idx, row in positions.iterrows():\n",
    "        pos.append({\n",
    "            \"id\": int(idx+1),\n",
    "            \"position\": {\n",
    "                \"x\": float(row.x),\n",
    "                \"y\": float(row.y)\n",
    "            }\n",
    "        })\n",
    "    with open('../backend/data/mainTfIdf/mainTfIdf.05112018.position.umap.json', \"w\") as f:\n",
    "        json.dump(pos, f)\n",
    "\n",
    "def mockTrainingForDemo(iterations):\n",
    "    data = loadData()\n",
    "    test_data = getTestData()\n",
    "    saveData(getSelectionScores(data, test_data))\n",
    "    resetTrainData()\n",
    "    mockTraining(iterations)\n",
    "    updateDatasetJson()\n",
    "    print('Done')\n",
    "    \n",
    "def updatePositions():\n",
    "    data = loadData()\n",
    "    tfidf = vec.fit_transform(data.text)\n",
    "    positions = applyDR(tfidf, withPreviousPos=False, labels=data.label)\n",
    "    write_position(positions=positions, algorithm='umap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mockTrainingForDemo(300)\n",
    "updatePositions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-1.1026\t17             \t\t3.9792\tmusik          \n",
      "\t-0.9990\tanmeldung      \t\t3.6341\tdj             \n",
      "\t-0.9250\tevents         \t\t2.4215\tkonzert        \n",
      "\t-0.8763\ttreffen        \t\t2.0582\tband           \n",
      "\t-0.8740\tbitte          \t\t2.0481\trock           \n",
      "\t-0.8729\tfilm           \t\t1.9503\tsongs          \n",
      "\t-0.8634\trumanischer    \t\t1.7841\tmusic          \n",
      "\t-0.8606\tspiele         \t\t1.6879\tkaraoke        \n",
      "\t-0.7872\ttag            \t\t1.5612\tlive           \n",
      "\t-0.7359\ttel            \t\t1.5502\teintritt       \n",
      "\t-0.7268\tglucksrad      \t\t1.2317\tmusikalischer  \n",
      "\t-0.7265\tjeder          \t\t1.2147\talbum          \n",
      "\t-0.6688\tarctic         \t\t1.1633\ttechno         \n",
      "\t-0.6688\teifeleck       \t\t1.1379\tmusikalische   \n",
      "\t-0.6377\tworkshop       \t\t1.1356\tmusiziert      \n",
      "\t-0.6281\twir            \t\t1.1294\tmusiker        \n",
      "\t-0.6264\tbuffet         \t\t1.1239\tacoustic       \n",
      "\t-0.6182\tkegelbahn      \t\t1.1191\tsound          \n",
      "\t-0.6017\tstammtisch     \t\t1.1160\tjazz           \n",
      "\t-0.5962\tliest          \t\t1.1031\tnacht          \n"
     ]
    }
   ],
   "source": [
    "analyseImportantFeatures(SGD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
