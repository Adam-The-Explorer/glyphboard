{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ducanh.trinh\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.manifold import MDS, TSNE, LocallyLinearEmbedding, Isomap\n",
    "from MulticoreTSNE import MulticoreTSNE\n",
    "from sklearn.decomposition import TruncatedSVD, PCA, KernelPCA, SparsePCA\n",
    "import umap\n",
    "from scipy.sparse import vstack\n",
    "import pandas as pd\n",
    "from gb_writer import GlyphboardWriter\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from typing import Any\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "SGD = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3,\n",
    "                    random_state=42, max_iter=5, tol=None)\n",
    "MNB = MultinomialNB()\n",
    "LR = LogisticRegression()\n",
    "SVC = LinearSVC()\n",
    "KNC = KNeighborsClassifier()\n",
    "NC = NearestCentroid()\n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "vec = TfidfVectorizer(strip_accents='ascii', max_df=0.5, sublinear_tf=True)\n",
    "SPLICE_POINT = 800\n",
    "UNLABELED_VALUE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    print('Updating JSON...')\n",
    "    updateDatasetJson()\n",
    "    # print('Cleaning Texts...')\n",
    "    # cleanupTexts()\n",
    "    print('Done')\n",
    "\n",
    "def mockInit():\n",
    "    texts = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    peer_labels = []\n",
    "    with open(\"test_data.json\", \"r\") as read_file:\n",
    "        LC_data = json.load(read_file)\n",
    "\n",
    "    for doc in LC_data:\n",
    "        ids.append(doc['id'])\n",
    "        texts.append(doc[\"values\"][\"7\"])\n",
    "        peer_labels.append(doc[\"features\"][\"1\"][\"4\"])\n",
    "        # simulate all as labeled (for test_data)\n",
    "        if (doc[\"features\"][\"1\"][\"4\"] > 0.5):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'text': texts,\n",
    "        'label': labels,\n",
    "        'peer_label': peer_labels,\n",
    "        'score': [0] * len(LC_data),\n",
    "        'isLabeled': [0]  * len(LC_data)\n",
    "    })\n",
    "   \n",
    "    test_data = df[SPLICE_POINT+1:]\n",
    "    saveData(test_data, 'test_data')\n",
    "    # test_data.to_csv('test_data.csv', sep=\";\", encoding=\"utf8\", index=False)\n",
    "    data_with_scores = getSelectionScores(rest_data=df)\n",
    "    saveData(data_with_scores)\n",
    "    resetTrainData()\n",
    "    \n",
    "    # data_with_scores.to_csv('data.csv', sep=\";\", encoding=\"utf8\", index=False)\n",
    "    # resetTrainData()\n",
    "\n",
    "def loadData(name = 'data'):\n",
    "    return pd.read_csv('{}.csv'.format(name), sep=\";\", encoding=\"utf8\")\n",
    "\n",
    "def saveData(data, name = 'data'):\n",
    "    data.to_csv('{}.csv'.format(name), sep=\";\", encoding=\"utf8\", index=False)\n",
    "\n",
    "def handleNewAnswer(answer):\n",
    "    newAnswer = {\n",
    "        'text': answer['text'],\n",
    "        'docId': answer['documentId'],\n",
    "        'label': int(answer['answer']),\n",
    "        'question': answer['questionId']\n",
    "    }\n",
    "    train_data = getTrainData()\n",
    "\n",
    "    test_data = getTestData()\n",
    "    \n",
    "    data = updateDataWithLabel(loadData(), newAnswer['docId'], newAnswer['label'])\n",
    "    if len(train_data) > 3:\n",
    "        # tfidf = vec.fit_transform(data.text)        \n",
    "        # positions = applyDR(tfidf, withPreviousPos=False, labels=data.label)\n",
    "        # writer = GlyphboardWriter('test_name')\n",
    "        # position_response = writer.write_position(positions=positions, algorithm='umap')\n",
    "        train_result = train(train_data, test_data, SGD)\n",
    "        return {\n",
    "            # 'positions': position_response,\n",
    "            'train_result': train_result\n",
    "        }\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def handleCompleteUpdate():\n",
    "    data = loadData()\n",
    "    # updateDatasetJson()\n",
    "    tfidf = vec.fit_transform(data.text)\n",
    "    positions = applyDR(tfidf, withPreviousPos=True, labels=data.label)\n",
    "    writer = GlyphboardWriter('test_name')\n",
    "    position_response = writer.write_position(positions=positions, algorithm='umap')\n",
    "    return position_response\n",
    "\n",
    "def updateDatasetJson():\n",
    "    with open(\"test_data.json\", \"r\") as read_file:\n",
    "        LC_data = json.load(read_file)\n",
    "        \n",
    "    data = loadData()\n",
    "\n",
    "    for doc in LC_data:\n",
    "        doc['features']['1']['31'] = int(data.loc[data['id'] == doc['id']].isLabeled.values[0])\n",
    "        doc['values']['31'] = int(data.loc[data['id'] == doc['id']].isLabeled.values[0])\n",
    "        doc['features']['1']['32'] = data.loc[data['id'] == doc['id']].score.values[0]\n",
    "        doc['values']['32'] = data.loc[data['id'] == doc['id']].score.values[0]\n",
    "        doc['features']['1']['33'] = int(data.loc[data['id'] == doc['id']].label.values[0])\n",
    "        doc['values']['33'] = int(data.loc[data['id'] == doc['id']].label.values[0])\n",
    "\n",
    "    with open(\"backend/data/mainTfIdf/mainTfIdf.05112018.feature.json\", \"w\") as f:\n",
    "            json.dump(LC_data, f)\n",
    "    \n",
    "    return 'Done'\n",
    "\n",
    "# def updateSingleData(id: number, label, isLabeled):\n",
    "#     data = loadData()\n",
    "#     json = \n",
    "\n",
    "def updateDataWithLabel(data, docId, label):\n",
    "    print('before', data.loc[data['id'] == docId])\n",
    "    data.loc[data['id'] == docId, 'label'] = int(label)\n",
    "    data.loc[data['id'] == docId, 'isLabeled'] = 1\n",
    "    print('after', data.loc[data['id'] == docId])\n",
    "    saveData(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def createMetrics(algo):\n",
    "    test_data = getTestData()\n",
    "    train_data_df = getTrainData()\n",
    "    train_data_df.label = train_data_df.label.astype(int)\n",
    "    met = []\n",
    "    # Create stepwise metrics algo, simulating a history\n",
    "    for number in range(30, len(train_data_df)):\n",
    "        train_data_iteration = train_data_df.head(number)\n",
    "        met.append(train(train_data_iteration, test_data, algo=algo))\n",
    "    return pd.DataFrame(met)\n",
    "\n",
    "\n",
    "def train(train_data, test_data, algo: Any) -> dict:\n",
    "    text_clf = Pipeline([\n",
    "        # ('vect', CountVectorizer()),\n",
    "        ('tfidf', vec),\n",
    "        ('clf', algo),\n",
    "    ])\n",
    "    text_clf.fit(train_data.text, train_data.label)\n",
    "    predicted = text_clf.predict(test_data.text)\n",
    "    addHistory(metrics.f1_score(test_data.label, predicted))\n",
    "    result = {\n",
    "        'precision': metrics.precision_score(test_data.label, predicted),\n",
    "        'recall': metrics.recall_score(test_data.label, predicted),\n",
    "        'f1': metrics.f1_score(test_data.label, predicted),\n",
    "        'f1_history': getHistory()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def getTrainData():\n",
    "    data = loadData()\n",
    "    return data.loc[data['isLabeled'] == 1]\n",
    "\n",
    "def getTestData():\n",
    "    return pd.read_csv('test_data.csv', delimiter=';', encoding=\"utf8\")\n",
    "\n",
    "def resetTrainData():\n",
    "    data = loadData()\n",
    "    data.loc[:, 'label'] = UNLABELED_VALUE\n",
    "    data.loc[:, 'isLabeled'] = 0\n",
    "    saveData(data)\n",
    "\n",
    "def cleanupTexts():\n",
    "    data = loadData()\n",
    "    for idx, text in enumerate(data.text):\n",
    "        data.loc[idx, 'text'] = preprocessText(text)\n",
    "        \n",
    "    saveData(data)\n",
    "\n",
    "def mockTraining(amount):\n",
    "    data = loadData()\n",
    "    for i in range(amount):\n",
    "        data.loc[i, 'isLabeled'] = 1\n",
    "        if data.loc[i].peer_label > 0.5:\n",
    "            data.loc[i, 'label'] = 1\n",
    "        else:\n",
    "            data.loc[i, 'label'] = 0        \n",
    "    saveData(data)\n",
    "\n",
    "def simulateTraining(iterations):\n",
    "    test_data = getTestData()\n",
    "    mockTraining(iterations)\n",
    "    train_data = getTrainData()\n",
    "    train(train_data, test_data, SGD)\n",
    "\n",
    "def getHistory():\n",
    "    history = []\n",
    "    with open(\n",
    "            \"metrics.csv\", \"r\", encoding=\"utf8\") as file:\n",
    "        reader = csv.reader(file, delimiter=';')\n",
    "        for line in reader:\n",
    "            history.append(line[0])\n",
    "        file.close()\n",
    "    return history\n",
    "\n",
    "\n",
    "def addHistory(metrics):\n",
    "    with open(\n",
    "            \"metrics.csv\", \"a\",  newline=\"\", encoding=\"utf8\") as file:\n",
    "        writer = csv.writer(file, delimiter=';')\n",
    "        writer.writerow([str(metrics)])\n",
    "        file.close()\n",
    "\n",
    "def getCurrentScore() -> int:\n",
    "    return getHistory().pop()\n",
    "\n",
    "def applyDR(tfidf, labels = [], withPreviousPos = True, factor = 1):    \n",
    "    # pre_computed = TruncatedSVD(n_components=100, random_state=1).fit_transform(tfidf.toarray())\n",
    "    # LABEL_IMPACT = 0\n",
    "    if withPreviousPos:        \n",
    "        previousPositions = loadData('previousPositions').values\n",
    "    else:\n",
    "        previousPositions = 'spectral'\n",
    "    labels_arr = np.asarray(labels)\n",
    "    labels_arr = labels_arr.reshape(len(labels_arr), 1)\n",
    "    # with_labels = np.hstack((tfidf.toarray(), labels_arr))\n",
    "    computed_coords = umap.UMAP(init=previousPositions,min_dist=0.8, random_state=1, learning_rate=0.5).fit(tfidf.toarray(), y=labels_arr)\n",
    "    computed_coords = computed_coords.embedding_\n",
    "    saveData(pd.DataFrame(computed_coords), 'previousPositions')\n",
    "    computed_coords *= factor    \n",
    "    # computed_coords = MulticoreTSNE(n_jobs=4, random_state=1).fit_transform(with_labels)\n",
    "    df = pd.DataFrame(columns=['x', 'y'])\n",
    "    df['x'] = computed_coords[:, 0]\n",
    "    df['y'] = computed_coords[:, 1]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# def resetPositions():\n",
    "\n",
    "\n",
    "def preprocessText(text: str) -> str:\n",
    "    # print('Original: ', text)\n",
    "    doc = nlp(text)\n",
    "    # Remove Stop Words and get Lemmas\n",
    "    return ' '.join([token.text for token in doc if not token.is_stop])\n",
    "    # for word in doc:\n",
    "    #     if word.is_stop == True:\n",
    "    #         print('Stop %s', word)\n",
    "    # print(word.lemma_)\n",
    "\n",
    "#             # Get NER\n",
    "#     for ent in doc.ents:\n",
    "#         print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "def getSelectionScores(clf = MNB, rest_data = loadData(), train_data = getTestData()): \n",
    "    text_clf = Pipeline([\n",
    "        # ('vect', CountVectorizer()),\n",
    "        ('tfidf', vec),\n",
    "        ('clf', clf),\n",
    "    ])\n",
    "    text_clf.fit(train_data.text, train_data.label)\n",
    "    prs = text_clf.predict_proba(rest_data.text) \n",
    "    result_pos = [1-2*abs(x[1]-0.5)  for x in prs]\n",
    "    rest_data['score'] = result_pos\n",
    "    return rest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updateDatasetJson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mockTraining(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>isLabeled</th>\n",
       "      <th>label</th>\n",
       "      <th>peer_label</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>295</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.995374</td>\n",
       "      <td>GARDENS END Sommerspecial - Die Rockband Garde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.497636</td>\n",
       "      <td>Sand in der Ritze #2 Nach einer gemütlichen un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>541</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.684377</td>\n",
       "      <td>The next Gamestorm Cocktail! Der Gamestorm bek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.991680</td>\n",
       "      <td>Klassisches Konzert Costabell Romantischer Kla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>803</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.68750</td>\n",
       "      <td>0.927309</td>\n",
       "      <td>Sommerakademie 2014 \"Mystik  und Alltag im int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>948</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>0.956348</td>\n",
       "      <td>Jazz im Weingut Pix Jazz im Weingut Pix\\r\\r\\r\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  isLabeled  label  peer_label     score  \\\n",
       "294  295          1      1     1.00000  0.995374   \n",
       "398  399          1      1     0.00000  0.497636   \n",
       "540  541          1      0     0.00000  0.684377   \n",
       "666  667          1      1     1.00000  0.991680   \n",
       "802  803          1      0     0.68750  0.927309   \n",
       "947  948          1      1     0.96875  0.956348   \n",
       "\n",
       "                                                  text  \n",
       "294  GARDENS END Sommerspecial - Die Rockband Garde...  \n",
       "398  Sand in der Ritze #2 Nach einer gemütlichen un...  \n",
       "540  The next Gamestorm Cocktail! Der Gamestorm bek...  \n",
       "666  Klassisches Konzert Costabell Romantischer Kla...  \n",
       "802  Sommerakademie 2014 \"Mystik  und Alltag im int...  \n",
       "947  Jazz im Weingut Pix Jazz im Weingut Pix\\r\\r\\r\\...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTrainData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleanupTexts()\n",
    "data = loadData()\n",
    "tfidf = vec.fit_transform(data.text)\n",
    "positions = applyDR(tfidf, labels=np.empty(1118), withPreviousPos=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'backend/data/mainTfIdf/mainTfIdf.05112018.position.umap.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-b266d0d9f847>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGlyphboardWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mposition_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpositions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'umap'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\projects\\glyphboard\\mlbackend\\gb_writer.py\u001b[0m in \u001b[0;36mwrite_position\u001b[1;34m(self, positions, algorithm)\u001b[0m\n\u001b[0;32m     55\u001b[0m                 }\n\u001b[0;32m     56\u001b[0m             })\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'backend/data/mainTfIdf/mainTfIdf.05112018.position.umap.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'backend/data/mainTfIdf/mainTfIdf.05112018.position.umap.json'"
     ]
    }
   ],
   "source": [
    "writer = GlyphboardWriter('test_name')\n",
    "position_response = writer.write_position(positions=positions, algorithm='umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resetTrainData()\n",
    "mockTraining(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating JSON...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'backend/data/mainTfIdf/mainTfIdf.05112018.feature.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-ecd396ae1e7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-ee72d3ca090f>\u001b[0m in \u001b[0;36minit\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Updating JSON...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mupdateDatasetJson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# print('Cleaning Texts...')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# cleanupTexts()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-ee72d3ca090f>\u001b[0m in \u001b[0;36mupdateDatasetJson\u001b[1;34m()\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'values'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'33'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"backend/data/mainTfIdf/mainTfIdf.05112018.feature.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLC_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'backend/data/mainTfIdf/mainTfIdf.05112018.feature.json'"
     ]
    }
   ],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetTrainData()\n",
    "# mockTraining(500)\n",
    "# SGD_met = createMetrics(SGD)\n",
    "# MNB_met = createMetrics(MNB)\n",
    "# KNC_met = createMetrics(KNC)\n",
    "# NC_met = createMetrics(NC)\n",
    "RFC_met = createMetrics(RFC)\n",
    "# getTrainData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SGD_met.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MNB_met.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNC_met.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NC_met.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_met.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
