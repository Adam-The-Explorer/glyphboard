{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import MDS, TSNE, LocallyLinearEmbedding, Isomap\n",
    "from MulticoreTSNE import MulticoreTSNE\n",
    "from sklearn.decomposition import TruncatedSVD, PCA, KernelPCA, SparsePCA\n",
    "import umap\n",
    "import pandas as pd\n",
    "from gb_writer import GlyphboardWriter\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from typing import Any\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "# import spacy\n",
    "# from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "# nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestData():\n",
    "    with open(\"test_data.json\", \"r\") as read_file:\n",
    "        test_data = json.load(read_file)\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for doc in test_data:\n",
    "    #     # only use confident labels\n",
    "         if (doc[\"features\"][\"1\"][\"4\"] > 0.5):\n",
    "            test_labels.append(1)\n",
    "            test_texts.append(doc[\"values\"][\"7\"])\n",
    "         else:\n",
    "             test_labels.append(0)\n",
    "             test_texts.append(doc[\"values\"][\"7\"])\n",
    "    return pd.DataFrame({'text': test_texts, 'label': test_labels})\n",
    "\n",
    "def train(train_data, test_data, algo: Any) -> dict:\n",
    "    # clean_data = [preprocessText(text) for text in data]\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        # ('nlp', preprocessText())\n",
    "        # ('clf', LogisticRegression()),\n",
    "        # ('clf', MultinomialNB()),\n",
    "        ('clf', algo),\n",
    "    ])\n",
    "    text_clf.fit(train_data.text, train_data.label)\n",
    "    # text_clf.fit(clean_data, labels)\n",
    "    predicted = text_clf.predict(test_data.text)\n",
    "    # print(dataframe.prediction.value_counts())\n",
    "    # print(metrics.classification_report(test_labels, predicted))\n",
    "    addHistory(metrics.f1_score(test_data.label, predicted))\n",
    "    result = {\n",
    "        # 'prediction': predicted.toList(),\n",
    "        # 'accuracy': metrics.accuracy_score(test_labels, predicted),\n",
    "        'precision': metrics.precision_score(test_data.label, predicted),\n",
    "        'recall': metrics.recall_score(test_data.label, predicted),\n",
    "        'f1': metrics.f1_score(test_data.label, predicted),\n",
    "        'f1_history': getHistory()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def getHistory():\n",
    "    history = []\n",
    "    with open(\n",
    "            \"mlbackend/metrics.csv\", \"r\", encoding=\"utf8\") as file:\n",
    "        reader = csv.reader(file, delimiter=';')\n",
    "        for line in reader:\n",
    "            history.append(line[0])\n",
    "        file.close()\n",
    "    return history\n",
    "\n",
    "def addHistory(metrics):\n",
    "    with open(\n",
    "            \"mlbackend/metrics.csv\", \"a\",  newline=\"\", encoding=\"utf8\") as file:\n",
    "        writer = csv.writer(file, delimiter=';')\n",
    "        writer.writerow([str(metrics)])\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getTestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDR(tfidf, labels):\n",
    "    # mds = MDS(n_components=2, random_state=1).fit_transform(tfidf.toarray())\n",
    "    lsi = TruncatedSVD(n_components=100, random_state=1).fit_transform(tfidf.toarray())\n",
    "    # mds = MDS(n_components=2, random_state=1).fit_transform(lsi)\n",
    "    labels = labels.reshape(len(labels), 1)\n",
    "    with_labels = np.hstack((lsi,labels))\n",
    "    with_labels.shape\n",
    "    mds = MDS(n_components=2, random_state=1).fit_transform(with_labels)\n",
    "#     mds = TSNE(n_components=2, random_state=1).fit_transform(with_labels)\n",
    "    df = pd.DataFrame(columns=['x', 'y'])\n",
    "    df['x'] = mds[:, 0]\n",
    "    df['y'] = mds[:, 1]\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getTestData()\n",
    "vec = TfidfVectorizer()\n",
    "tfidf = vec.fit_transform(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             x         y\n",
      "0     7.506356  1.401674\n",
      "1    -3.612138 -2.039203\n",
      "2     0.478525 -5.534179\n",
      "3    -4.284943  2.448015\n",
      "4     0.889897 -1.649368\n",
      "5    -4.903683 -3.059757\n",
      "6    -6.948209  0.938133\n",
      "7     0.536590 -6.534554\n",
      "8    -6.884484  7.115571\n",
      "9    -3.871984 -4.557209\n",
      "10    7.921367  0.130527\n",
      "11   -0.959576  5.622871\n",
      "12    6.262791 -4.693450\n",
      "13    6.628671 -2.361881\n",
      "14   -7.039403  6.142621\n",
      "15    1.305112 -6.200580\n",
      "16    7.474166 -3.319695\n",
      "17   -6.099412  2.795892\n",
      "18   -0.830204 -6.461778\n",
      "19   -7.006938  0.670415\n",
      "20   -1.570214 -2.628725\n",
      "21    3.324758 -6.201512\n",
      "22    1.430066 -6.286922\n",
      "23   -0.806018  5.878713\n",
      "24    6.487830 -0.327867\n",
      "25   -1.429619 -3.714862\n",
      "26   -5.294108 -0.819282\n",
      "27    3.279274 -5.504846\n",
      "28    7.839597  1.452218\n",
      "29    0.370711 -3.867836\n",
      "...        ...       ...\n",
      "1088 -4.631029 -2.740127\n",
      "1089 -0.455486 -3.913275\n",
      "1090 -1.507559 -4.510925\n",
      "1091  6.201534 -4.752929\n",
      "1092 -6.304682  5.091229\n",
      "1093 -2.570395 -4.474161\n",
      "1094  0.967407 -4.657150\n",
      "1095  6.079129 -4.846052\n",
      "1096 -3.877024 -4.084750\n",
      "1097 -2.706620 -3.759723\n",
      "1098 -4.505857 -0.948288\n",
      "1099  2.981391 -5.155778\n",
      "1100  6.477985 -4.554041\n",
      "1101  0.607956 -5.653451\n",
      "1102 -0.874861 -2.012246\n",
      "1103 -1.451070  5.148940\n",
      "1104  1.097234 -5.683516\n",
      "1105 -7.125892  2.236903\n",
      "1106  1.254313 -6.158231\n",
      "1107  2.547959 -5.996613\n",
      "1108 -5.678845  1.201782\n",
      "1109 -3.613803 -0.067797\n",
      "1110 -2.192228 -5.664547\n",
      "1111  6.796460  1.186705\n",
      "1112 -3.287297  0.104942\n",
      "1113  6.884135 -2.636017\n",
      "1114  6.234893 -0.217571\n",
      "1115  6.860760 -1.381422\n",
      "1116  6.541120  1.041861\n",
      "1117 -5.411538  1.219386\n",
      "\n",
      "[1118 rows x 2 columns]\n",
      "Writing positions...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "lsi = TruncatedSVD(n_components=2, random_state=1).fit_transform(tfidf.toarray())\n",
    "# lsi = PCA(n_components=10, random_state=1).fit_transform(tfidf.toarray())\n",
    "\n",
    "labels = np.asarray(data.label)\n",
    "labels = labels.reshape(len(labels), 1)\n",
    "with_labels = np.hstack((lsi,labels))\n",
    "\n",
    "\n",
    "\n",
    "# m = Sequential()\n",
    "# m.add(Dense(512,  activation='elu', input_shape=(101,)))\n",
    "# m.add(Dense(128,  activation='elu'))\n",
    "# m.add(Dense(2,    activation='linear', name=\"bottleneck\"))\n",
    "# m.add(Dense(128,  activation='elu'))\n",
    "# m.add(Dense(512,  activation='elu'))\n",
    "# m.add(Dense(101,  activation='sigmoid'))\n",
    "# m.compile(loss='mean_squared_error', optimizer = Adam())\n",
    "# history = m.fit(with_labels, with_labels, batch_size=128, epochs=5, verbose=1)\n",
    "\n",
    "# encoder = Model(m.input, m.get_layer('bottleneck').output)\n",
    "# lsi_with_labels = encoder.predict(with_labels)\n",
    "\n",
    "# lsi_with_labels = PCA(n_components=2, random_state=1).fit_transform(with_labels)\n",
    "# lsi_with_labels = TruncatedSVD(n_components=2, random_state=1).fit_transform(with_labels)\n",
    "# lsi_with_labels = Isomap(n_components=2).fit_transform(with_labels)\n",
    "# lsi_with_labels = MulticoreTSNE(n_jobs=1, n_components=2, random_state=1).fit_transform(with_labels)\n",
    "# lsi_with_labels = LocallyLinearEmbedding(n_jobs=4,n_components=2, random_state=1).fit_transform(with_labels)\n",
    "lsi_with_labels = umap.UMAP(random_state=1).fit_transform(lsi)\n",
    "df = pd.DataFrame(columns=['x', 'y'])\n",
    "df['x'] = lsi_with_labels[:, 0]\n",
    "df['y'] = lsi_with_labels[:, 1]\n",
    "    \n",
    "    \n",
    "df *= 1\n",
    "# DR.x *= 500\n",
    "# DR.y *= 500\n",
    "\n",
    "print(df)\n",
    "\n",
    "writer = GlyphboardWriter('test_name')\n",
    "\n",
    "# DR *= 2\n",
    "print('Writing positions...')    \n",
    "writer.write_position(df, 'lsi')\n",
    "del df\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data.json\", \"r\") as read_file:\n",
    "        test_data = json.load(read_file)\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "ids = []\n",
    "\n",
    "for doc in test_data:\n",
    "    ids.append(doc['id'])\n",
    "    test_labels.append(1)\n",
    "    test_texts.append(doc[\"values\"][\"7\"])\n",
    "\n",
    "df = pd.DataFrame({'id': ids, 'text': test_texts, 'label': test_labels})\n",
    "df.to_csv('data.csv', sep=\";\")\n",
    "# return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
